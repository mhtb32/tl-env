{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Goal Environment\n",
    "\n",
    "Here we want to train an agent to reach a goal using reinforcement learning algorithms.\n",
    "\n",
    "## Warm-up\n",
    "First, we do some installations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "\n",
    "!apt-get install graphviz libgraphviz-dev pkg-config\n",
    "!apt-get install -y xvfb python-opengl ffmpeg\n",
    "!pip install git+https://github.com/mhtb32/tl-env.git#egg=tl-env\n",
    "!pip install stable-baselines==2.10.0 pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we do imports and initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import gym\n",
    "from stable_baselines.sac.policies import MlpPolicy\n",
    "from stable_baselines import SAC\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "# noinspection PyUnresolvedReferences\n",
    "import tl_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we specify a save path to save trained model and make the environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(Path.cwd().parent / 'out').mkdir(exist_ok=True)\n",
    "save_path = Path.cwd().parent / 'out'\n",
    "\n",
    "env = gym.make('tl_env:SingleGoalIDM-v0')\n",
    "eval_env = gym.make('tl_env:SingleGoalIDM-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now we train the agent using Soft Actor Critic(SAC) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(eval_env, eval_freq=3000, best_model_save_path=str(save_path))\n",
    "\n",
    "model = SAC(MlpPolicy, env, verbose=1, buffer_size=10000)\n",
    "model.learn(total_timesteps=40000, log_interval=200, callback=eval_callback)\n",
    "model.save(str(save_path / 'final_model_40000'))\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Now we test the agent for a few episodes to see how it is doing. We first define a simple helper function for\n",
    "visualization of episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "from gym.wrappers import Monitor\n",
    "import base64\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "def show_video():\n",
    "    html = []\n",
    "    for mp4 in Path(\"../out/video\").glob(\"*.mp4\"):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append('''<video alt=\"{}\" autoplay\n",
    "                      loop controls style=\"height: 400px;\">\n",
    "                      <source src=\"data:../out/video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "    # noinspection PyTypeChecker\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = Monitor(eval_env, '../out/video', force=True, video_callable=lambda episode: True)\n",
    "model = SAC.load(str(save_path / 'best_model'))\n",
    "for episode in trange(3, desc=\"Test episodes\"):\n",
    "    obs, done = env.reset(), False\n",
    "    env.unwrapped.automatic_rendering_callback = env.video_recorder.capture_frame\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Finally, we evaluate the policy to have a quantitative sense of how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"10-episode reward is {mean_reward:.2f} +/- {std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}